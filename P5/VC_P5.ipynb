{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar herramientas de c++https://visualstudio.microsoft.com/es/visual-cpp-build-tools/\n",
    "# Descargar el modelo desde: http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
    "\n",
    "# Ruta al modelo preentrenado de predicción de 68 puntos de landmarks\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "# Cargar el detector de rostros y el predictor de landmarks\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "\n",
    "# Path to the facial landmarks predictor model\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "# Load face detector and landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Load Chopper's hat image with a transparent background\n",
    "chopper_hat = cv2.imread(\"chopper_hat.png\", cv2.IMREAD_UNCHANGED)  # Complete path to Chopper's hat\n",
    "\n",
    "# Check if the image loaded correctly\n",
    "if chopper_hat is None:\n",
    "    print(\"Error: The image 'chopper_hat.png' could not be loaded. Check the file path.\")\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    exit(1)\n",
    "\n",
    "def overlay_image(background, overlay, pos_x, pos_y):\n",
    "    \"\"\"\n",
    "    Overlay an image with an alpha channel over another image at a specific position,\n",
    "    ensuring it does not go out of bounds of the frame.\n",
    "    \"\"\"\n",
    "    h, w = overlay.shape[:2]\n",
    "\n",
    "    # Crop the overlay if it goes out of the frame bounds\n",
    "    if pos_y < 0:\n",
    "        overlay = overlay[-pos_y:, :]\n",
    "        pos_y = 0\n",
    "    if pos_x < 0:\n",
    "        overlay = overlay[:, -pos_x:]\n",
    "        pos_x = 0\n",
    "    if pos_y + h > background.shape[0]:\n",
    "        overlay = overlay[:background.shape[0] - pos_y, :]\n",
    "    if pos_x + w > background.shape[1]:\n",
    "        overlay = overlay[:, :background.shape[1] - pos_x]\n",
    "\n",
    "    # Apply the overlay using the alpha channel\n",
    "    alpha_overlay = overlay[:, :, 3] / 255.0  # Alpha channel for transparency\n",
    "    for c in range(0, 3):\n",
    "        background[pos_y:pos_y + overlay.shape[0], pos_x:pos_x + overlay.shape[1], c] = \\\n",
    "            (1 - alpha_overlay) * background[pos_y:pos_y + overlay.shape[0], pos_x:pos_x + overlay.shape[1], c] + \\\n",
    "            alpha_overlay * overlay[:, :, c]\n",
    "    return background\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    try:\n",
    "        faces = DeepFace.extract_faces(frame, detector_backend='yolov8')\n",
    "        \n",
    "        for face in faces:\n",
    "            x, y = face['facial_area']['x'], face['facial_area']['y']\n",
    "            w, h = face['facial_area']['w'], face['facial_area']['h']\n",
    "            dlib_rect = dlib.rectangle(left=x, top=y, right=x + w, bottom=y + h)\n",
    "            \n",
    "            \n",
    "            landmarks = predictor(gray, dlib_rect)\n",
    "            \n",
    "            # Get key points for eyes, temples, and chin for hat size and position\n",
    "            left_eye = (landmarks.part(36).x, landmarks.part(36).y)\n",
    "            right_eye = (landmarks.part(45).x, landmarks.part(45).y)\n",
    "            left_temple = (landmarks.part(17).x, landmarks.part(17).y)  # Left temple point\n",
    "            right_temple = (landmarks.part(26).x, landmarks.part(26).y)  # Right temple point\n",
    "            chin = (landmarks.part(8).x, landmarks.part(8).y)\n",
    "\n",
    "            # Calculate width and height for the hat\n",
    "            head_width = int(2.5 * (right_temple[0] - left_temple[0]))  # Increased width for better coverage\n",
    "            head_height = int(1.5 * abs(chin[1] - left_eye[1]))   # Adjusted height for appropriate look\n",
    "\n",
    "            # Resize Chopper's hat\n",
    "            resized_chopper_hat = cv2.resize(chopper_hat, (head_width, head_height))\n",
    "            \n",
    "            # Position the hat: centered over the forehead and head\n",
    "            hat_pos_x = left_temple[0] - (head_width *2// 7)  # Center horizontally\n",
    "            hat_pos_y = left_eye[1] - int(head_height * 0.95)  # Position higher to cover the top of the head\n",
    "\n",
    "            # Overlay the hat on the image\n",
    "            overlay_image(frame, resized_chopper_hat, hat_pos_x, hat_pos_y)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Display the image with Chopper's hat overlay\n",
    "    cv2.imshow(\"One Piece Filter\", frame)\n",
    "    if cv2.waitKey(5) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'luffy_hat.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m yolo_model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov8n.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Adjust model as needed\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Load images for each filter item\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m luffy_hat \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mluffy_hat.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMREAD_UNCHANGED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m zoro_scar \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzoro_scar.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mIMREAD_UNCHANGED)\n\u001b[0;32m     17\u001b[0m curly_eyebrow \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurly_eyebrow.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mIMREAD_UNCHANGED)\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P5\\Lib\\site-packages\\ultralytics\\utils\\patches.py:26\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(filename, flags)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m, flags: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR):\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    Read an image from a file.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m        (np.ndarray): The read image.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mimdecode(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m, flags)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'luffy_hat.png'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from deepface import DeepFace\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load YOLOv8 model for face detection\n",
    "yolo_model = YOLO(\"yolov8n.pt\")  # Adjust model as needed\n",
    "\n",
    "# Load images for each filter item\n",
    "luffy_hat = cv2.imread(\"luffy_hat.png\", cv2.IMREAD_UNCHANGED)\n",
    "zoro_scar = cv2.imread(\"zoro_scar.png\", cv2.IMREAD_UNCHANGED)\n",
    "curly_eyebrow = cv2.imread(\"curly_eyebrow.png\", cv2.IMREAD_UNCHANGED)\n",
    "brook_afro = cv2.imread(\"brook_afro.png\", cv2.IMREAD_UNCHANGED)\n",
    "chopper_hat = cv2.imread(\"chopper_hat.png\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# Function to overlay image with rotation and scaling\n",
    "def overlay_rotated_image(background, overlay, pos, angle=0, scale=1.0):\n",
    "    h, w = overlay.shape[:2]\n",
    "    overlay = cv2.resize(overlay, (int(w * scale), int(h * scale)))\n",
    "    center = (overlay.shape[1] // 2, overlay.shape[0] // 2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated_overlay = cv2.warpAffine(overlay, rotation_matrix, (overlay.shape[1], overlay.shape[0]), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)\n",
    "    overlay_x, overlay_y = pos\n",
    "\n",
    "    for y in range(rotated_overlay.shape[0]):\n",
    "        for x in range(rotated_overlay.shape[1]):\n",
    "            if overlay_y + y >= background.shape[0] or overlay_x + x >= background.shape[1]:\n",
    "                continue\n",
    "            alpha = rotated_overlay[y, x, 3] / 255.0\n",
    "            background[overlay_y + y, overlay_x + x] = (1 - alpha) * background[overlay_y + y, overlay_x + x] + alpha * rotated_overlay[y, x, :3]\n",
    "    return background\n",
    "\n",
    "# Main loop\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect faces using YOLOv8\n",
    "    results = yolo_model(frame)\n",
    "    faces = []\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            faces.append((x1, y1, x2, y2))\n",
    "\n",
    "    # Process each detected face\n",
    "    for facial_area in faces:\n",
    "        # Extract landmarks with DeepFace’s RetinaFace backend\n",
    "        landmarks = None\n",
    "        try:\n",
    "            analysis = DeepFace.analyze(frame, actions=['emotion'], detector_backend=\"retinaface\")\n",
    "            landmarks = analysis[0][\"region\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error detecting landmarks: {e}\")\n",
    "\n",
    "        if landmarks:\n",
    "            # Position and scale Luffy's Hat\n",
    "            left_temple = (landmarks[\"left_eye_x\"], landmarks[\"left_eye_y\"] - 20)\n",
    "            right_temple = (landmarks[\"right_eye_x\"], landmarks[\"right_eye_y\"] - 20)\n",
    "            hat_x = min(left_temple[0], right_temple[0]) - int(luffy_hat.shape[1] * 0.7)\n",
    "            hat_y = left_temple[1] - luffy_hat.shape[0] // 2\n",
    "            overlay_rotated_image(frame, luffy_hat, (hat_x, hat_y), scale=0.6)\n",
    "\n",
    "            # Position and scale Zoro's Eye Scar\n",
    "            scar_x = landmarks[\"left_eye_x\"] - int(zoro_scar.shape[1] * 0.5)\n",
    "            scar_y = landmarks[\"left_eye_y\"]\n",
    "            overlay_rotated_image(frame, zoro_scar, (scar_x, scar_y), scale=0.5)\n",
    "\n",
    "            # Position and scale Sanji’s Curly Eyebrows\n",
    "            eyebrow_scale = 0.6\n",
    "            eyebrow_x_left = landmarks[\"left_eye_x\"] - int(curly_eyebrow.shape[1] * eyebrow_scale // 2)\n",
    "            eyebrow_y_left = landmarks[\"left_eye_y\"] - 15\n",
    "            eyebrow_x_right = landmarks[\"right_eye_x\"] - int(curly_eyebrow.shape[1] * eyebrow_scale // 2)\n",
    "            eyebrow_y_right = landmarks[\"right_eye_y\"] - 15\n",
    "            overlay_rotated_image(frame, curly_eyebrow, (eyebrow_x_left, eyebrow_y_left), scale=eyebrow_scale)\n",
    "            overlay_rotated_image(frame, curly_eyebrow, (eyebrow_x_right, eyebrow_y_right), scale=eyebrow_scale)\n",
    "\n",
    "            # Position and scale Brook’s Afro\n",
    "            afro_width = int((right_temple[0] - left_temple[0]) * 1.5)\n",
    "            afro_height = int(afro_width * brook_afro.shape[0] / brook_afro.shape[1])\n",
    "            resized_afro = cv2.resize(brook_afro, (afro_width, afro_height))\n",
    "            afro_x = left_temple[0] - afro_width // 4\n",
    "            afro_y = left_temple[1] - afro_height // 2\n",
    "            overlay_rotated_image(frame, resized_afro, (afro_x, afro_y))\n",
    "\n",
    "            # Position and scale Chopper's Hat and Antlers\n",
    "            hat_x_chopper = min(left_temple[0], right_temple[0]) - int(chopper_hat.shape[1] * 0.7)\n",
    "            hat_y_chopper = left_temple[1] - chopper_hat.shape[0] // 2\n",
    "            overlay_rotated_image(frame, chopper_hat, (hat_x_chopper, hat_y_chopper), scale=0.6)\n",
    "\n",
    "    # Display the frame with filters\n",
    "    cv2.imshow(\"One Piece Filters\", frame)\n",
    "    if cv2.waitKey(5) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
