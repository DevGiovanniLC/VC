{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar herramientas de c++https://visualstudio.microsoft.com/es/visual-cpp-build-tools/\n",
    "# Descargar el modelo desde: http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
    "import cv2\n",
    "import dlib\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "img_list = []\n",
    "\n",
    "def overlay_image(background, overlay, pos_x, pos_y):\n",
    "    \"\"\"\n",
    "    Overlay an image with an alpha channel over another image at a specific position,\n",
    "    ensuring it does not go out of bounds of the frame.\n",
    "    \"\"\"\n",
    "    h, w = overlay.shape[:2]\n",
    "\n",
    "    # Crop the overlay if it goes out of the frame bounds\n",
    "    if pos_y < 0:\n",
    "        overlay = overlay[-pos_y:, :]\n",
    "        pos_y = 0\n",
    "    if pos_x < 0:\n",
    "        overlay = overlay[:, -pos_x:]\n",
    "        pos_x = 0\n",
    "    if pos_y + h > background.shape[0]:\n",
    "        overlay = overlay[:background.shape[0] - pos_y, :]\n",
    "    if pos_x + w > background.shape[1]:\n",
    "        overlay = overlay[:, :background.shape[1] - pos_x]\n",
    "\n",
    "    # Apply the overlay using the alpha channel\n",
    "    alpha_overlay = overlay[:, :, 3] / 255.0  # Alpha channel for transparency\n",
    "    for c in range(0, 3):\n",
    "        background[pos_y:pos_y + overlay.shape[0], pos_x:pos_x + overlay.shape[1], c] = \\\n",
    "            (1 - alpha_overlay) * background[pos_y:pos_y + overlay.shape[0], pos_x:pos_x + overlay.shape[1], c] + \\\n",
    "            alpha_overlay * overlay[:, :, c]\n",
    "    return background\n",
    "\n",
    "def load_image(path):\n",
    "    return cv2.imread(path, cv2.IMREAD_UNCHANGED)  # Complete path to Chopper's hat\n",
    "\n",
    "\n",
    "\n",
    "def choper_hat_filter(frame, landmarks):\n",
    "    chopper_hat = load_image(\"chopper_hat.png\")\n",
    "\n",
    "    left_eye = (landmarks.part(36).x, landmarks.part(36).y)\n",
    "    left_temple = (landmarks.part(17).x, landmarks.part(17).y)  # Left temple point\n",
    "    right_temple = (landmarks.part(26).x, landmarks.part(26).y)  # Right temple point\n",
    "    chin = (landmarks.part(8).x, landmarks.part(8).y)\n",
    "\n",
    "    head_width = int(2.5 * (right_temple[0] - left_temple[0]))  # Increased width for better coverage\n",
    "    head_height = int(1.5 * abs(chin[1] - left_eye[1]))   # Adjusted height for appropriate look\n",
    "    chopper_hat = cv2.resize(chopper_hat, (head_width, head_height))\n",
    "    \n",
    "    # Position the hat: centered over the forehead and head\n",
    "    hat_pos_x = left_temple[0] - (head_width *2 // 7)  # Center horizontally\n",
    "    hat_pos_y = left_eye[1] - int(head_height * 0.95)  # Position higher to cover the top of the head\n",
    "\n",
    "    # Overlay the hat on the image\n",
    "    overlay_image(frame, chopper_hat, hat_pos_x, hat_pos_y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predictor = dlib.shape_predictor( \"shape_predictor_68_face_landmarks.dat\")\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    try:\n",
    "        faces = DeepFace.extract_faces(frame, detector_backend='yolov8')\n",
    "        \n",
    "        for face in faces:\n",
    "            x, y = face['facial_area']['x'], face['facial_area']['y']\n",
    "            w, h = face['facial_area']['w'], face['facial_area']['h']\n",
    "            dlib_rect = dlib.rectangle(left=x, top=y, right=x + w, bottom=y + h)\n",
    "            \n",
    "            landmarks = predictor(gray, dlib_rect)\n",
    "            \n",
    "            choper_hat_filter(frame, landmarks)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Display the image with Chopper's hat overlay\n",
    "    cv2.imshow(\"One Piece Filter\", frame)\n",
    "    if cv2.waitKey(5) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from deepface import DeepFace\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load YOLOv8 model for face detection\n",
    "yolo_model = YOLO(\"yolov8n.pt\")  # Adjust model as needed\n",
    "\n",
    "# Load images for each filter item\n",
    "luffy_hat = cv2.imread(\"luffy_hat.png\", cv2.IMREAD_UNCHANGED)\n",
    "luffy_scar = cv2.imread(\"luffy_scar.png\", cv2.IMREAD_UNCHANGED)\n",
    "zoro_scar = cv2.imread(\"zoro_scar.png\", cv2.IMREAD_UNCHANGED)\n",
    "sanji_eyebrow= cv2.imread(\"sanji_eyebrow.png\", cv2.IMREAD_UNCHANGED)\n",
    "chopper_hat = cv2.imread(\"chopper_hat.png\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# Function to overlay image with rotation and scaling\n",
    "def overlay_rotated_image(background, overlay, pos, angle=0, scale=1.0):\n",
    "    h, w = overlay.shape[:2]\n",
    "    overlay = cv2.resize(overlay, (int(w * scale), int(h * scale)))\n",
    "    center = (overlay.shape[1] // 2, overlay.shape[0] // 2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated_overlay = cv2.warpAffine(overlay, rotation_matrix, (overlay.shape[1], overlay.shape[0]), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)\n",
    "    overlay_x, overlay_y = pos\n",
    "\n",
    "    for y in range(rotated_overlay.shape[0]):\n",
    "        for x in range(rotated_overlay.shape[1]):\n",
    "            if overlay_y + y >= background.shape[0] or overlay_x + x >= background.shape[1]:\n",
    "                continue\n",
    "            alpha = rotated_overlay[y, x, 3] / 255.0\n",
    "            background[overlay_y + y, overlay_x + x] = (1 - alpha) * background[overlay_y + y, overlay_x + x] + alpha * rotated_overlay[y, x, :3]\n",
    "    return background\n",
    "\n",
    "# Main loop\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect faces using YOLOv8\n",
    "    results = yolo_model(frame)\n",
    "    faces = []\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            faces.append((x1, y1, x2, y2))\n",
    "\n",
    "    # Process each detected face\n",
    "    for facial_area in faces:\n",
    "        # Extract landmarks with DeepFace’s RetinaFace backend\n",
    "        landmarks = None\n",
    "        try:\n",
    "            analysis = DeepFace.analyze(frame, actions=['emotion'], detector_backend=\"retinaface\")\n",
    "            landmarks = analysis[0][\"region\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error detecting landmarks: {e}\")\n",
    "\n",
    "        if landmarks:\n",
    "            # Position and scale Luffy's Hat\n",
    "            left_temple = (landmarks[\"left_eye_x\"], landmarks[\"left_eye_y\"] - 20)\n",
    "            right_temple = (landmarks[\"right_eye_x\"], landmarks[\"right_eye_y\"] - 20)\n",
    "            hat_x = min(left_temple[0], right_temple[0]) - int(luffy_hat.shape[1] * 0.7)\n",
    "            hat_y = left_temple[1] - luffy_hat.shape[0] // 2\n",
    "            overlay_rotated_image(frame, luffy_hat, (hat_x, hat_y), scale=0.6)\n",
    "\n",
    "            # Position and scale Zoro's Eye Scar\n",
    "            scar_x = landmarks[\"left_eye_x\"] - int(zoro_scar.shape[1] * 0.5)\n",
    "            scar_y = landmarks[\"left_eye_y\"]\n",
    "            overlay_rotated_image(frame, zoro_scar, (scar_x, scar_y), scale=0.5)\n",
    "\n",
    "            # Position and scale Sanji’s Curly Eyebrows\n",
    "            eyebrow_scale = 0.6\n",
    "            eyebrow_x_left = landmarks[\"left_eye_x\"] - int(curly_eyebrow.shape[1] * eyebrow_scale // 2)\n",
    "            eyebrow_y_left = landmarks[\"left_eye_y\"] - 15\n",
    "            eyebrow_x_right = landmarks[\"right_eye_x\"] - int(curly_eyebrow.shape[1] * eyebrow_scale // 2)\n",
    "            eyebrow_y_right = landmarks[\"right_eye_y\"] - 15\n",
    "            overlay_rotated_image(frame, curly_eyebrow, (eyebrow_x_left, eyebrow_y_left), scale=eyebrow_scale)\n",
    "            overlay_rotated_image(frame, curly_eyebrow, (eyebrow_x_right, eyebrow_y_right), scale=eyebrow_scale)\n",
    "\n",
    "\n",
    "            # Position and scale Chopper's Hat and Antlers\n",
    "            hat_x_chopper = min(left_temple[0], right_temple[0]) - int(chopper_hat.shape[1] * 0.7)\n",
    "            hat_y_chopper = left_temple[1] - chopper_hat.shape[0] // 2\n",
    "            overlay_rotated_image(frame, chopper_hat, (hat_x_chopper, hat_y_chopper), scale=0.6)\n",
    "\n",
    "    # Display the frame with filters\n",
    "    cv2.imshow(\"One Piece Filters\", frame)\n",
    "    if cv2.waitKey(5) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
