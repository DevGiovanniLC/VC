{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import easyocr\n",
    "\n",
    "# Crear el lector de EasyOCR para el idioma adecuado\n",
    "reader = easyocr.Reader(['en'])  \n",
    "\n",
    "root_dir = \"./datasets/matriculas-europeas\"\n",
    "subdirs = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# Crear directorios de etiquetas si no existen\n",
    "for subdir in subdirs:\n",
    "    label_dir = os.path.join(root_dir, subdir, \"labels\")\n",
    "    os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "# Función para obtener el cuadro delimitador de la matrícula usando EasyOCR\n",
    "def detectar_matricula_easyocr(imagen):\n",
    "    # Preprocesamiento de la imagen\n",
    "    imagen_gray = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)\n",
    "    imagen_contrast = cv2.convertScaleAbs(imagen_gray, alpha=1.3, beta=15)  # Ajuste de contraste y brillo\n",
    "    resultados = reader.readtext(imagen_contrast, detail=1)\n",
    "\n",
    "    # Filtrar resultados para encontrar el texto más probable que sea una matrícula\n",
    "    for (bbox, texto, _) in resultados:\n",
    "        texto_limpio = ''.join(filter(str.isalnum, texto))  # Quitar espacios y caracteres especiales\n",
    "        if 5 <= len(texto_limpio) <= 10:  # Ajuste según longitud esperada de una matrícula\n",
    "            x_min = min(bbox[0][0], bbox[1][0], bbox[2][0], bbox[3][0])\n",
    "            y_min = min(bbox[0][1], bbox[1][1], bbox[2][1], bbox[3][1])\n",
    "            x_max = max(bbox[0][0], bbox[1][0], bbox[2][0], bbox[3][0])\n",
    "            y_max = max(bbox[0][1], bbox[1][1], bbox[2][1], bbox[3][1])\n",
    "            width = x_max - x_min\n",
    "            height = y_max - y_min\n",
    "            return x_min, y_min, width, height\n",
    "    return None  \n",
    "\n",
    "# Procesar cada conjunto de datos\n",
    "for subdir in subdirs:\n",
    "    img_dir = os.path.join(root_dir, subdir, \"images\")\n",
    "    label_dir = os.path.join(root_dir, subdir, \"labels\")\n",
    "\n",
    "    for img_path in glob.glob(f\"{img_dir}/*.[jp][pn]g\"):\n",
    "        img_name = os.path.basename(img_path).split('.')[0]\n",
    "        \n",
    "        # Leer la imagen\n",
    "        img = cv2.imread(img_path)\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        # Intentar detectar matrícula usando EasyOCR\n",
    "        ocr_box = detectar_matricula_easyocr(img)\n",
    "        \n",
    "        if ocr_box:\n",
    "            # Obtener coordenadas y dimensiones normalizadas para formato YOLO\n",
    "            x, y, w, h = ocr_box\n",
    "            x_center = (x + w / 2) / width\n",
    "            y_center = (y + h / 2) / height\n",
    "            width_norm = w / width\n",
    "            height_norm = h / height\n",
    "        else:\n",
    "            # Si EasyOCR no detecta, intentar una zona común para la matrícula (parte baja de la imagen)\n",
    "            print(f\"No se detectó matrícula en {img_name}. Usando zona predeterminada.\")\n",
    "            x, y = int(width * 0.2), int(height * 0.75)  # Zona estimada para la matrícula\n",
    "            w, h = int(width * 0.6), int(height * 0.2)\n",
    "            x_center = (x + w / 2) / width\n",
    "            y_center = (y + h / 2) / height\n",
    "            width_norm = w / width\n",
    "            height_norm = h / height\n",
    "\n",
    "        # Guardar el archivo de etiqueta en formato YOLO\n",
    "        label_file = os.path.join(label_dir, f\"{img_name}.txt\")\n",
    "        with open(label_file, 'w') as f:\n",
    "            f.write(f\"0 {x_center} {y_center} {width_norm} {height_norm}\\n\")\n",
    "\n",
    "        print(f\"Etiqueta generada para {img_name} en {label_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from ultralytics.models.yolo.detect import DetectionTrainer\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Verifica si se puede usar una GPU\n",
    "print(\"GPU disponible: \", torch.cuda.is_available())\n",
    "\n",
    "# Parámetros de entrenamiento para el modelo\n",
    "args = dict(\n",
    "    model=\"yolo11n.pt\",  # Modelo YOLO preentrenado\n",
    "    data=\"license_plates.yaml\",  # Archivo de configuración de datos\n",
    "    epochs=150,  # Aumentar el número de épocas\n",
    "    batch=64,  # Tamaño del lote ajustado\n",
    "    imgsz=640,  # Tamaño de imagen (ajustable según GPU)\n",
    "    multi_scale=True,  # Habilita escalado múltiple para variar el tamaño de imagen\n",
    "    workers=4,  # Número de trabajadores para el DataLoader\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",  # Usar GPU si está disponible, de lo contrario CPU\n",
    "    verbose=True,  # Mostrar progreso de entrenamiento\n",
    "    lr0=0.001,  # Tasa de aprendizaje inicial ajustada\n",
    "    weight_decay=0.001,  # Regularización L2\n",
    "    patience=25,  # Early stopping después de 20épocas sin mejora\n",
    "    augment=True,  # Habilitar aumento de datos\n",
    ")\n",
    "\n",
    "# Inicializa el entrenador y entrena el modelo\n",
    "trainer = DetectionTrainer(overrides=args)  \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_plate = YOLO('./runs/detect/train15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado: 5951FPF, Probabilidad: 0.9998589687661023\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Función para mostrar la imagen\n",
    "def mostrar_imagen(imagen):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Inicializar el lector OCR de EasyOCR\n",
    "lector = easyocr.Reader([\"en\"], gpu=True)\n",
    "\n",
    "def postprocesar_imagen(imagen):\n",
    "    if imagen is None or imagen.size == 0:\n",
    "        return None\n",
    "    if len(imagen.shape) < 2 or imagen.shape[0] == 0 or imagen.shape[1] == 0:\n",
    "        print(\"Error: La imagen no tiene las dimensiones adecuadas.\")\n",
    "        return None\n",
    "    imagen_blur = cv2.GaussianBlur(imagen.copy(), (1, 1), 0)\n",
    "    imagen_contrast = cv2.convertScaleAbs(imagen_blur, alpha=1.1, beta=15)\n",
    "    return imagen_contrast\n",
    "\n",
    "def detectar_texto(imagen_procesada):\n",
    "    if imagen_procesada is None or imagen_procesada.size == 0:\n",
    "        return imagen_procesada, \"\", 0.01\n",
    "    \n",
    "    resultado = lector.readtext(imagen_procesada, detail=1)\n",
    "    arriba_izq = (0, 0)\n",
    "    abajo_der = (0, 0)\n",
    "    count = 0\n",
    "    texto, probabilidad = \"\", 0.01\n",
    "\n",
    "    for bbox, texto, probabilidad in resultado:\n",
    "        (arriba_izq_actual, _, abajo_der_actual, _) = bbox\n",
    "        arriba_izq_actual = tuple([int(val) for val in arriba_izq_actual])\n",
    "        abajo_der_actual = tuple([int(val) for val in abajo_der_actual])\n",
    "\n",
    "        if count == 0:\n",
    "            arriba_izq = arriba_izq_actual\n",
    "            abajo_der = abajo_der_actual\n",
    "            count += 1\n",
    "        else:\n",
    "            abajo_der = abajo_der_actual\n",
    "\n",
    "    if not resultado:\n",
    "        return imagen_procesada, \"\", 0.01\n",
    "\n",
    "    # Recortar la región de interés con las validaciones para evitar errores\n",
    "    height, width = imagen_procesada.shape[:2]\n",
    "    arriba_izq = (max(0, arriba_izq[0]), max(0, arriba_izq[1]))\n",
    "    abajo_der = (min(width, abajo_der[0]), min(height, abajo_der[1]))\n",
    "\n",
    "    roi_plate = imagen_procesada[arriba_izq[1]:abajo_der[1], arriba_izq[0]:abajo_der[0]]\n",
    "    \n",
    "    return roi_plate, texto, probabilidad\n",
    "\n",
    "def procesar_deteccion(imagen_procesada):\n",
    "    if imagen_procesada is None or imagen_procesada.size == 0:\n",
    "        print(\"Error: Imagen procesada es None o está vacía.\")\n",
    "        return None, \"\", 0.01\n",
    "\n",
    "    # Inicializar variables con valores predeterminados\n",
    "    imagen_anterior = imagen_procesada\n",
    "    texto_anterior = \"\"\n",
    "    probabilidad_anterior = 0.01\n",
    "\n",
    "    resultado = detectar_texto(imagen_procesada)\n",
    "    # Verificar si detectar_texto devolvió resultados válidos\n",
    "    if resultado:\n",
    "        imagen_actual, texto_actual, probabilidad_actual = resultado\n",
    "    else:\n",
    "        return imagen_anterior, texto_anterior, probabilidad_anterior  # Devolver los valores iniciales si no hay detección\n",
    "\n",
    "    # Ciclo de detección y actualización de variables\n",
    "    while probabilidad_actual > probabilidad_anterior:\n",
    "        probabilidad_anterior = probabilidad_actual\n",
    "        imagen_anterior = imagen_actual\n",
    "        texto_anterior = texto_actual\n",
    "        resultado = detectar_texto(imagen_anterior)\n",
    "        if resultado:\n",
    "            imagen_actual, texto_actual, probabilidad_actual = resultado\n",
    "        else:\n",
    "            break  # Salir del bucle si no se obtienen nuevos resultados válidos\n",
    "\n",
    "    return imagen_anterior, texto_anterior, probabilidad_anterior\n",
    "\n",
    "\n",
    "def preprocesar_imagen(imagen):\n",
    "    if imagen is None or imagen.size == 0:\n",
    "        return None\n",
    "    if len(imagen.shape) < 2 or imagen.shape[0] == 0 or imagen.shape[1] == 0:\n",
    "        print(\"Error: La imagen no tiene las dimensiones adecuadas.\")\n",
    "        return None\n",
    "    imagen_blur = cv2.GaussianBlur(imagen, (5, 5), 0)\n",
    "    imagen_gray = cv2.cvtColor(imagen_blur, cv2.COLOR_BGR2GRAY)\n",
    "    imagen_contrast = cv2.convertScaleAbs(imagen_gray, alpha=1.1, beta=10)\n",
    "    _, imagen_binaria = cv2.threshold(imagen_contrast, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return imagen_binaria\n",
    "\n",
    "def procesar_imagen(imagen_procesada):\n",
    "    if imagen_procesada is None or imagen_procesada.size == 0:\n",
    "        print(\"Error: Imagen procesada es None o está vacía.\")\n",
    "        return None\n",
    "    if len(imagen_procesada.shape) < 2 or imagen_procesada.shape[0] == 0 or imagen_procesada.shape[1] == 0:\n",
    "        print(\"Error: La imagen no tiene las dimensiones adecuadas.\")\n",
    "        return None\n",
    "\n",
    "    imagen_actual, texto_actual, probabilidad_actual = procesar_deteccion(imagen_procesada)\n",
    "    probabilidad_anterior = 0\n",
    "    imagen_anterior = \"\"\n",
    "    texto_anterior = \"\"\n",
    "\n",
    "    while probabilidad_actual > probabilidad_anterior:\n",
    "        probabilidad_anterior = probabilidad_actual\n",
    "        imagen_anterior = imagen_actual\n",
    "        texto_anterior = texto_actual\n",
    "        post_procesado = postprocesar_imagen(imagen_actual)\n",
    "        \n",
    "        # Agregar chequeo para evitar procesar una imagen nula\n",
    "        if post_procesado is None:\n",
    "            break\n",
    "            \n",
    "        imagen_actual, texto_actual, probabilidad_actual = procesar_deteccion(post_procesado)\n",
    "\n",
    "    if probabilidad_anterior > 0.01 and 3 < len(texto_anterior) < 10:\n",
    "        return imagen_anterior, texto_anterior, probabilidad_anterior\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def OCR(imagen):\n",
    "    if imagen is None or imagen.size == 0:\n",
    "        print(\"Error: Imagen de entrada vacía.\")\n",
    "        return None\n",
    "    if len(imagen.shape) < 2 or imagen.shape[0] == 0 or imagen.shape[1] == 0:\n",
    "        print(\"Error: La imagen no tiene las dimensiones adecuadas.\")\n",
    "        return None\n",
    "    if len(imagen.shape) < 3 or imagen.shape[2] != 3:\n",
    "        print(\"Error: La imagen no tiene los canales de color necesarios.\")\n",
    "        return None\n",
    "    imagen_preprocesada = preprocesar_imagen(imagen.copy())\n",
    "    return procesar_imagen(imagen_preprocesada)\n",
    "\n",
    "# Cargar la imagen y ejecutar OCR\n",
    "image_path = \"./5951FPF.png\"\n",
    "imagen = cv2.imread(image_path)\n",
    "\n",
    "ocr_resultado = OCR(imagen)\n",
    "if ocr_resultado:\n",
    "    imagen_resultado, texto, probabilidad = ocr_resultado\n",
    "    if not probabilidad:\n",
    "        print(\"No se detectó ningún texto\")\n",
    "    else:\n",
    "        print(f\"Resultado: {texto}, Probabilidad: {probabilidad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible:  False\n",
      "WARNING  Environment does not support cv2.imshow() or PIL Image.show()\n",
      "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1301: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
      "\n",
      "Procesando fotograma 1\n",
      "Procesando fotograma 2\n",
      "Procesando fotograma 3\n",
      "Procesando fotograma 4\n",
      "Procesando fotograma 5\n",
      "Procesando fotograma 6\n",
      "Procesando fotograma 7\n",
      "Procesando fotograma 8\n",
      "Procesando fotograma 9\n",
      "Procesando fotograma 10\n",
      "Procesando fotograma 11\n",
      "Procesando fotograma 12\n",
      "Procesando fotograma 13\n",
      "Procesando fotograma 14\n",
      "Procesando fotograma 15\n",
      "Procesando fotograma 16\n",
      "Procesando fotograma 17\n",
      "Procesando fotograma 18\n",
      "Procesando fotograma 19\n",
      "Procesando fotograma 20\n",
      "Procesando fotograma 21\n",
      "Procesando fotograma 22\n",
      "Procesando fotograma 23\n",
      "Procesando fotograma 24\n",
      "Procesando fotograma 25\n",
      "Procesando fotograma 26\n",
      "Procesando fotograma 27\n",
      "Procesando fotograma 28\n",
      "Procesando fotograma 29\n",
      "Procesando fotograma 30\n",
      "Procesando fotograma 31\n",
      "Procesando fotograma 32\n",
      "Procesando fotograma 33\n",
      "Procesando fotograma 34\n",
      "Procesando fotograma 35\n",
      "Procesando fotograma 36\n",
      "Procesando fotograma 37\n",
      "Procesando fotograma 38\n",
      "Procesando fotograma 39\n",
      "Procesando fotograma 40\n",
      "Procesando fotograma 41\n",
      "Procesando fotograma 42\n",
      "Procesando fotograma 43\n",
      "Procesando fotograma 44\n",
      "Procesando fotograma 45\n",
      "Procesando fotograma 46\n",
      "Procesando fotograma 47\n",
      "Procesando fotograma 48\n",
      "Procesando fotograma 49\n",
      "Procesando fotograma 50\n",
      "Procesando fotograma 51\n",
      "Procesando fotograma 52\n",
      "Procesando fotograma 53\n",
      "Procesando fotograma 54\n",
      "Procesando fotograma 55\n",
      "Procesando fotograma 56\n",
      "Procesando fotograma 57\n",
      "Procesando fotograma 58\n",
      "Procesando fotograma 59\n",
      "Procesando fotograma 60\n",
      "Procesando fotograma 61\n",
      "Procesando fotograma 62\n",
      "Procesando fotograma 63\n",
      "Procesando fotograma 64\n",
      "Procesando fotograma 65\n",
      "Procesando fotograma 66\n",
      "Procesando fotograma 67\n",
      "Procesando fotograma 68\n",
      "Procesando fotograma 69\n",
      "Procesando fotograma 70\n",
      "Procesando fotograma 71\n",
      "Procesando fotograma 72\n",
      "Procesando fotograma 73\n",
      "Procesando fotograma 74\n",
      "Procesando fotograma 75\n",
      "Procesando fotograma 76\n",
      "Procesando fotograma 77\n",
      "Procesando fotograma 78\n",
      "Procesando fotograma 79\n",
      "Procesando fotograma 80\n",
      "Procesando fotograma 81\n",
      "Procesando fotograma 82\n",
      "Procesando fotograma 83\n",
      "Procesando fotograma 84\n",
      "Procesando fotograma 85\n",
      "Procesando fotograma 86\n",
      "Procesando fotograma 87\n",
      "Procesando fotograma 88\n",
      "Procesando fotograma 89\n",
      "Procesando fotograma 90\n",
      "Procesando fotograma 91\n",
      "Procesando fotograma 92\n",
      "Procesando fotograma 93\n",
      "Procesando fotograma 94\n",
      "Procesando fotograma 95\n",
      "Procesando fotograma 96\n",
      "Procesando fotograma 97\n",
      "Procesando fotograma 98\n",
      "Procesando fotograma 99\n",
      "Procesando fotograma 100\n",
      "Procesando fotograma 101\n",
      "Procesando fotograma 102\n",
      "Procesando fotograma 103\n",
      "Procesando fotograma 104\n",
      "Procesando fotograma 105\n",
      "Procesando fotograma 106\n",
      "Procesando fotograma 107\n",
      "Procesando fotograma 108\n",
      "Procesando fotograma 109\n",
      "Procesando fotograma 110\n",
      "Procesando fotograma 111\n",
      "Procesando fotograma 112\n",
      "Procesando fotograma 113\n",
      "Procesando fotograma 114\n",
      "Procesando fotograma 115\n",
      "Procesando fotograma 116\n",
      "Procesando fotograma 117\n",
      "Procesando fotograma 118\n",
      "Procesando fotograma 119\n",
      "Procesando fotograma 120\n",
      "Procesando fotograma 121\n",
      "Procesando fotograma 122\n",
      "Procesando fotograma 123\n",
      "Procesando fotograma 124\n",
      "Procesando fotograma 125\n",
      "Procesando fotograma 126\n",
      "Procesando fotograma 127\n",
      "Procesando fotograma 128\n",
      "Procesando fotograma 129\n",
      "Procesando fotograma 130\n",
      "Procesando fotograma 131\n",
      "Procesando fotograma 132\n",
      "Procesando fotograma 133\n",
      "Procesando fotograma 134\n",
      "Procesando fotograma 135\n",
      "Procesando fotograma 136\n",
      "Procesando fotograma 137\n",
      "Procesando fotograma 138\n",
      "Procesando fotograma 139\n",
      "Procesando fotograma 140\n",
      "Procesando fotograma 141\n",
      "Procesando fotograma 142\n",
      "Procesando fotograma 143\n",
      "Procesando fotograma 144\n",
      "Procesando fotograma 145\n",
      "Procesando fotograma 146\n",
      "Procesando fotograma 147\n",
      "Procesando fotograma 148\n",
      "Procesando fotograma 149\n",
      "Procesando fotograma 150\n",
      "Procesando fotograma 151\n",
      "Procesando fotograma 152\n",
      "Procesando fotograma 153\n",
      "Procesando fotograma 154\n",
      "Procesando fotograma 155\n",
      "Procesando fotograma 156\n",
      "Procesando fotograma 157\n",
      "Procesando fotograma 158\n",
      "Procesando fotograma 159\n",
      "Procesando fotograma 160\n",
      "Procesando fotograma 161\n",
      "Procesando fotograma 162\n",
      "Procesando fotograma 163\n",
      "Procesando fotograma 164\n",
      "Procesando fotograma 165\n",
      "Procesando fotograma 166\n",
      "Procesando fotograma 167\n",
      "Procesando fotograma 168\n",
      "Procesando fotograma 169\n",
      "Procesando fotograma 170\n",
      "Procesando fotograma 171\n",
      "Procesando fotograma 172\n",
      "Procesando fotograma 173\n",
      "Procesando fotograma 174\n",
      "Procesando fotograma 175\n",
      "Procesando fotograma 176\n",
      "Procesando fotograma 177\n",
      "Procesando fotograma 178\n",
      "Procesando fotograma 179\n",
      "Procesando fotograma 180\n",
      "Procesando fotograma 181\n",
      "Procesando fotograma 182\n",
      "Procesando fotograma 183\n",
      "Procesando fotograma 184\n",
      "Procesando fotograma 185\n",
      "Procesando fotograma 186\n",
      "Procesando fotograma 187\n",
      "Procesando fotograma 188\n",
      "Procesando fotograma 189\n",
      "Procesando fotograma 190\n",
      "Procesando fotograma 191\n",
      "Procesando fotograma 192\n",
      "Procesando fotograma 193\n",
      "Procesando fotograma 194\n",
      "Procesando fotograma 195\n",
      "Procesando fotograma 196\n",
      "Procesando fotograma 197\n",
      "Procesando fotograma 198\n",
      "Procesando fotograma 199\n",
      "Procesando fotograma 200\n",
      "Procesando fotograma 201\n",
      "Procesando fotograma 202\n",
      "Procesando fotograma 203\n",
      "Procesando fotograma 204\n",
      "Procesando fotograma 205\n",
      "Procesando fotograma 206\n",
      "Procesando fotograma 207\n",
      "Procesando fotograma 208\n",
      "Procesando fotograma 209\n",
      "Procesando fotograma 210\n",
      "Procesando fotograma 211\n",
      "Procesando fotograma 212\n",
      "Procesando fotograma 213\n",
      "Procesando fotograma 214\n",
      "Procesando fotograma 215\n",
      "Procesando fotograma 216\n",
      "Procesando fotograma 217\n",
      "Procesando fotograma 218\n",
      "Procesando fotograma 219\n",
      "Procesando fotograma 220\n",
      "Procesando fotograma 221\n",
      "Procesando fotograma 222\n",
      "Procesando fotograma 223\n",
      "Procesando fotograma 224\n",
      "Procesando fotograma 225\n",
      "Procesando fotograma 226\n",
      "Procesando fotograma 227\n",
      "Procesando fotograma 228\n",
      "Procesando fotograma 229\n",
      "Procesando fotograma 230\n",
      "Procesando fotograma 231\n",
      "Procesando fotograma 232\n",
      "Procesando fotograma 233\n",
      "Procesando fotograma 234\n",
      "Procesando fotograma 235\n",
      "Procesando fotograma 236\n",
      "Procesando fotograma 237\n",
      "Procesando fotograma 238\n",
      "Procesando fotograma 239\n",
      "Procesando fotograma 240\n",
      "Procesando fotograma 241\n",
      "Procesando fotograma 242\n",
      "Procesando fotograma 243\n",
      "Procesando fotograma 244\n",
      "Procesando fotograma 245\n",
      "Procesando fotograma 246\n",
      "Procesando fotograma 247\n",
      "Procesando fotograma 248\n",
      "Procesando fotograma 249\n",
      "Procesando fotograma 250\n",
      "Procesando fotograma 251\n",
      "Procesando fotograma 252\n",
      "Procesando fotograma 253\n",
      "Procesando fotograma 254\n",
      "Procesando fotograma 255\n",
      "Procesando fotograma 256\n",
      "Procesando fotograma 257\n",
      "Procesando fotograma 258\n",
      "Procesando fotograma 259\n",
      "Procesando fotograma 260\n",
      "Procesando fotograma 261\n",
      "Procesando fotograma 262\n",
      "Procesando fotograma 263\n",
      "Procesando fotograma 264\n",
      "Procesando fotograma 265\n",
      "Procesando fotograma 266\n",
      "Procesando fotograma 267\n",
      "Procesando fotograma 268\n",
      "Procesando fotograma 269\n",
      "Procesando fotograma 270\n",
      "Procesando fotograma 271\n",
      "Procesando fotograma 272\n",
      "Procesando fotograma 273\n",
      "Procesando fotograma 274\n",
      "Procesando fotograma 275\n",
      "Procesando fotograma 276\n",
      "Procesando fotograma 277\n",
      "Procesando fotograma 278\n",
      "Procesando fotograma 279\n",
      "Procesando fotograma 280\n",
      "Procesando fotograma 281\n",
      "Procesando fotograma 282\n",
      "Procesando fotograma 283\n",
      "Procesando fotograma 284\n",
      "Procesando fotograma 285\n",
      "Procesando fotograma 286\n",
      "Procesando fotograma 287\n",
      "Procesando fotograma 288\n",
      "Procesando fotograma 289\n",
      "Procesando fotograma 290\n",
      "Procesando fotograma 291\n",
      "Procesando fotograma 292\n",
      "Procesando fotograma 293\n",
      "Procesando fotograma 294\n",
      "Procesando fotograma 295\n",
      "Procesando fotograma 296\n",
      "Procesando fotograma 297\n",
      "Procesando fotograma 298\n",
      "Procesando fotograma 299\n",
      "Procesando fotograma 300\n",
      "Procesando fotograma 301\n",
      "Procesando fotograma 302\n",
      "Procesando fotograma 303\n",
      "Procesando fotograma 304\n",
      "Procesando fotograma 305\n",
      "Procesando fotograma 306\n",
      "Procesando fotograma 307\n",
      "Procesando fotograma 308\n",
      "Procesando fotograma 309\n",
      "Procesando fotograma 310\n",
      "Procesando fotograma 311\n",
      "Procesando fotograma 312\n",
      "Procesando fotograma 313\n",
      "Procesando fotograma 314\n",
      "Procesando fotograma 315\n",
      "Procesando fotograma 316\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 111\u001b[0m\n\u001b[0;32m    108\u001b[0m roi \u001b[38;5;241m=\u001b[39m frame[y1:y2, x1:x2]  \u001b[38;5;66;03m# Región de interés (ROI) del vehículo\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Usar el modelo de matrículas para detectar matrículas en la ROI\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m plate_results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_plate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Procesar los resultados de detección de matrículas\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m plate_result \u001b[38;5;129;01min\u001b[39;00m plate_results:\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\ultralytics\\engine\\model.py:554\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:254\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 254\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:142\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    137\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    138\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    141\u001b[0m )\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:465\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 465\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:112\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:130\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:151\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 151\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    152\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:187\u001b[0m, in \u001b[0;36mSPPF.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through Ghost Convolution block.\"\"\"\u001b[39;00m\n\u001b[0;32m    186\u001b[0m y \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)]\n\u001b[1;32m--> 187\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:187\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through Ghost Convolution block.\"\"\"\u001b[39;00m\n\u001b[0;32m    186\u001b[0m y \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)]\n\u001b[1;32m--> 187\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:213\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\torch\\_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\VC_P1\\Lib\\site-packages\\torch\\nn\\functional.py:830\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import easyocr\n",
    "import csv\n",
    "import logging\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Configura el logger para suprimir mensajes\n",
    "logging.getLogger('ultralytics').setLevel(logging.WARNING)\n",
    "\n",
    "# Verifica si se puede usar una GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"GPU disponible: \", torch.cuda.is_available())\n",
    "\n",
    "# Configuración de OCR\n",
    "reader = easyocr.Reader(['es'])\n",
    "\n",
    "classNames = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"bus\"]\n",
    "\n",
    "datos = {name: {\"from_front\": [], \"to_front\": []} for name in classNames}\n",
    "\n",
    "# Cargar los modelos YOLO\n",
    "model_general = YOLO('yolo11n.pt')  # Modelo para detectar personas y vehículos\n",
    "model_plate = YOLO('best.pt')  # Modelo para detectar matrículas\n",
    "\n",
    "# Configuración del archivo de video y CSV\n",
    "filename = \"C0142.MP4\"\n",
    "output_video_filename = \"resultado_deteccion.mp4\"\n",
    "csv_filename = \"resultados.csv\"\n",
    "\n",
    "# Inicializar el video y el escritor\n",
    "cap = cv2.VideoCapture(filename)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out_video = cv2.VideoWriter(output_video_filename, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "#detecta procedencia del objeto\n",
    "def detectar_direccion(track_id, label_name, x, frame):\n",
    "    if (\n",
    "        track_id not in datos[label_name][\"from_front\"]\n",
    "        and track_id not in datos[label_name][\"to_front\"]\n",
    "    ):\n",
    "        if label_name == \"person\" or label_name == \"bicycle\":\n",
    "            if (x) < frame.shape[1] * 0.2 or (frame.shape[1]*0.7< (x) < frame.shape[1] * 0.95):\n",
    "                datos[label_name][\"to_front\"].append(track_id)\n",
    "            else:\n",
    "                datos[label_name][\"from_front\"].append(track_id)\n",
    "        else:\n",
    "            if (x) < frame.shape[1] * 0.7:\n",
    "                datos[label_name][\"from_front\"].append(track_id)\n",
    "            elif (x) > frame.shape[1] * 0.9:\n",
    "                datos[label_name][\"to_front\"].append(track_id)\n",
    "        return ''\n",
    "    elif track_id in datos[label_name][\"to_front\"]:\n",
    "        return ''\n",
    "    elif track_id in datos[label_name][\"from_front\"]:\n",
    "        return ''\n",
    "    \n",
    "\n",
    "# Preparación del archivo CSV\n",
    "with open(csv_filename, mode='w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['fotograma', 'tipo_objeto', 'confianza', 'identificador_tracking', 'x1', 'y1', 'x2', 'y2',\n",
    "                        'matrícula_en_su_caso', 'texto_matricula'])\n",
    "\n",
    "    # Variables de conteo\n",
    "    conteo_clases = {name: 0 for name in classNames}\n",
    "\n",
    "    # Procesar el video con detección general\n",
    "    results = model_general.track(source=filename, show=True, stream=True, verbose=False)\n",
    "    frame_count = 0\n",
    "\n",
    "    # Almacenar matrículas y sus IDs para el seguimiento con su confianza\n",
    "    matrículas_seguimiento = {}\n",
    "\n",
    "    # Iterar sobre cada resultado por frame\n",
    "    for frame_result in results:\n",
    "        frame_count += 1\n",
    "\n",
    "        frame = frame_result.orig_img\n",
    "\n",
    "        # Imprimir fotograma actual y total de fotogramas\n",
    "        print(f'Procesando fotograma {frame_count}')\n",
    "\n",
    "        # Obtener detecciones y agregar track_id de cada objeto\n",
    "        for box in frame_result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            score = box.conf[0].item()\n",
    "            label = int(box.cls[0])\n",
    "            track_id = int(box.id[0]) if box.id is not None else -1\n",
    "\n",
    "            if score >= 0.4:  # Umbral de confianza\n",
    "                # Verificar si el label es válido para YOLO\n",
    "                if 0 <= label < len(classNames):\n",
    "                    label_name = classNames[label]\n",
    "\n",
    "                    # Aumentar el conteo para la clase correspondiente\n",
    "                    if(detectar_direccion(track_id, label_name, x2, frame) == None): continue\n",
    "\n",
    "                    # Dibujar el rectángulo y mostrar el ID de seguimiento en el objeto\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                    cv2.putText(frame, f'ID: {track_id}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "                    cv2.putText(frame, label_name, (x1, y1 - 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "\n",
    "                    # Procesar solo si es un vehículo para buscar matrículas\n",
    "                    if label_name in [\"car\", \"bus\", \"motorbike\"]: \n",
    "                        roi = frame[y1:y2, x1:x2]  # Región de interés (ROI) del vehículo\n",
    "\n",
    "                        # Usar el modelo de matrículas para detectar matrículas en la ROI\n",
    "                        plate_results = model_plate.predict(source=roi, show=False)  \n",
    "\n",
    "                        # Procesar los resultados de detección de matrículas\n",
    "                        for plate_result in plate_results:\n",
    "                            for plate_box in plate_result.boxes:\n",
    "                                plate_x1, plate_y1, plate_x2, plate_y2 = map(int, plate_box.xyxy[0])\n",
    "                                plate_score = plate_box.conf[0].item()\n",
    "\n",
    "                                if plate_score >= 0.3:  # Umbral de confianza para matrículas\n",
    "                                    # Dibuja el rectángulo de la matrícula en el frame\n",
    "                                    cv2.rectangle(frame, (plate_x1 + x1, plate_y1 + y1), (plate_x2 + x1, plate_y2 + y1), (0, 255, 0), 2)\n",
    "\n",
    "                                    # Leer el texto de la matrícula con preprocesamiento\n",
    "                                    roi_plate = roi[plate_y1:plate_y2, plate_x1:plate_x2]\n",
    "\n",
    "                                    # Aplicar OCR a la ROI de la matrícula\n",
    "\n",
    "                                    if roi_plate is None or roi_plate.size == 0 or len(imagen.shape) < 2 or imagen.shape[0] == 0 or imagen.shape[1] == 0:\n",
    "                                        print(\"Error: La región de interés (ROI) de la matrícula está vacía.\")\n",
    "                                    else:\n",
    "                                        ocr_results_plate = OCR(roi_plate)\n",
    "\n",
    "                                    if ocr_results_plate:\n",
    "                                        imagen_resultado, matricula_texto, ocr_confianza = ocr_results_plate\n",
    "\n",
    "                                        # Almacenar solo la matrícula con mayor confianza\n",
    "                                        if (track_id not in matrículas_seguimiento) or (ocr_confianza > matrículas_seguimiento[track_id][1]) or (abs(len(matrículas_seguimiento[track_id][0])-7) > abs(len(matricula_texto)-7)):\n",
    "                                            matrículas_seguimiento[track_id] = (matricula_texto, ocr_confianza)\n",
    "                                            print(f'Matrícula detectada en fotograma {frame_count}: {matricula_texto} con confianza {ocr_confianza}')\n",
    "\n",
    "                                    # Obtener la matrícula de mayor confianza para el CSV\n",
    "                                    texto_matricula = matrículas_seguimiento[track_id][0] if track_id in matrículas_seguimiento else \"N/A\"\n",
    "\n",
    "                                    # Escribir en el archivo CSV\n",
    "                                    csv_writer.writerow([frame_count, label_name, score, track_id, x1, y1, x2, y2,\n",
    "                                                        \"matrícula\", texto_matricula])\n",
    "\n",
    "                                    # Mostrar el texto de la matrícula en el video\n",
    "                                    cv2.putText(frame, texto_matricula, (plate_x1 + x1, plate_y1 + y1 - 10), \n",
    "                                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "                                    break  # Romper después de procesar la primera matrícula detectada\n",
    "                        else:\n",
    "                            # Si no se detectó ninguna matrícula\n",
    "                            csv_writer.writerow([frame_count, label_name, score, track_id, x1, y1, x2, y2,\n",
    "                                                \"N/A\", \"N/A\"])\n",
    "                    else:\n",
    "                        # Escribir en el archivo CSV sin matrícula para objetos que no son vehículos\n",
    "                        csv_writer.writerow([frame_count, label_name, score, track_id, x1, y1, x2, y2,\n",
    "                                            \"N/A\", \"N/A\"])\n",
    "\n",
    "                    # Mostrar la matrícula correspondiente si ya fue detectada con el texto de mayor confianza\n",
    "                    if track_id in matrículas_seguimiento:\n",
    "                        cv2.putText(frame, matrículas_seguimiento[track_id][0], (x1, y1 - 40), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Guardar el frame procesado en el archivo de video\n",
    "        out_video.write(frame)\n",
    "\n",
    "# Liberar recursos\n",
    "cap.release()\n",
    "out_video.release()\n",
    "\n",
    "# Mostrar conteo de clases\n",
    "\n",
    "conteo_clases = 0\n",
    "for object, data in datos.items():\n",
    "    conteo_clases += len(data['from_front']) + len(data['to_front'])\n",
    "    print(\n",
    "        f\"{object}: \\n\\t{len(data['from_front'])} vienen de frente, \\n\\t{len(data['to_front'])} vienen de atras, \\n\\ttotal: {len(data['from_front']) + len(data['to_front'])}\"\n",
    "    )\n",
    "print(\"Conteo total de objetos: \", conteo_clases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Asegurarse de que las gráficas se muestren en el notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Leer el archivo CSV de resultados\n",
    "results_csv_path = './runs/detect/train4/results.csv'\n",
    "results_df = pd.read_csv(results_csv_path)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame y las columnas disponibles\n",
    "print(results_df.head())\n",
    "print(\"Columnas disponibles:\", results_df.columns)\n",
    "\n",
    "# Verificar si las columnas necesarias existen\n",
    "required_columns = ['epoch', 'val/box_loss', 'train/box_loss', 'metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
    "for col in required_columns:\n",
    "    if col not in results_df.columns:\n",
    "        print(f\"Error: La columna {col} no existe en el DataFrame.\")\n",
    "        exit()\n",
    "\n",
    "# Graficar las métricas de entrenamiento y validación\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results_df['epoch'], results_df['val/box_loss'], label='Validation Box Loss')\n",
    "plt.plot(results_df['epoch'], results_df['train/box_loss'], label='Training Box Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Box Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results_df['epoch'], results_df['metrics/precision(B)'], label='Precision')\n",
    "plt.plot(results_df['epoch'], results_df['metrics/recall(B)'], label='Recall')\n",
    "plt.plot(results_df['epoch'], results_df['metrics/mAP50(B)'], label='mAP50')\n",
    "plt.plot(results_df['epoch'], results_df['metrics/mAP50-95(B)'], label='mAP50-95')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metrics')\n",
    "plt.legend()\n",
    "plt.title('Precision, Recall, and mAP Metrics')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
